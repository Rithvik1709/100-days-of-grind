{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca ML Model Evaluation - Breast Cancer Dataset\n", "This notebook evaluates a machine learning model using various metrics like Accuracy, Precision, Recall, F1-Score, and AUC-ROC."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import (\n", "    accuracy_score, precision_score, recall_score, f1_score, classification_report, \n", "    roc_curve, roc_auc_score\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcc2 Load Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load dataset\n", "df = pd.read_csv('../data/breast_cancer_data.csv')\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcca Splitting Data & Training Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df.drop(columns=['target'])\n", "y = df['target']\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "# Train a RandomForest Model\n", "model = RandomForestClassifier(n_estimators=100, random_state=42)\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 Model Evaluation Metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Predictions\n", "y_pred = model.predict(X_test)\n", "\n", "# Compute Metrics\n", "accuracy = accuracy_score(y_test, y_pred)\n", "precision = precision_score(y_test, y_pred)\n", "recall = recall_score(y_test, y_pred)\n", "f1 = f1_score(y_test, y_pred)\n", "\n", "print(f'Accuracy: {accuracy:.2f}')\n", "print(f'Precision: {precision:.2f}')\n", "print(f'Recall: {recall:.2f}')\n", "print(f'F1-Score: {f1:.2f}')\n", "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcc8 AUC-ROC Curve"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute ROC Curve\n", "y_probs = model.predict_proba(X_test)[:, 1]\n", "fpr, tpr, _ = roc_curve(y_test, y_probs)\n", "auc_score = roc_auc_score(y_test, y_probs)\n", "\n", "# Plot ROC Curve\n", "plt.figure(figsize=(8,6))\n", "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n", "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal Line\n", "plt.xlabel('False Positive Rate (FPR)')\n", "plt.ylabel('True Positive Rate (TPR)')\n", "plt.title('ROC Curve')\n", "plt.legend()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}